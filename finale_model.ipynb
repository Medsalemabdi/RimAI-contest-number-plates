{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Final model:\n",
        "this notebook contain the final model wich is a combination between the yolo models and the easyocr model,it recognizes the plate using yolo then crop it and read it using easyocr\n",
        "there two ways tor run the model :\n",
        "\n",
        "*   predicting one image using predict_one function\n",
        "*   predicting many images using predict_many function\n",
        "\n"
      ],
      "metadata": {
        "id": "fbjeFYu8yKDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# merging both models in one function wich takes a photo path and prints the prediction string\n",
        "\n",
        "def predict_one(img_path):\n",
        "  model = YOLO('/content/drive/MyDrive/TrainYolov8CustomDataset/best (1).pt')\n",
        "  image = cv2.imread(img_path)\n",
        "  if image is None:\n",
        "        print(f\"Error: Could not load image at path: {path}\")\n",
        "        return\n",
        "\n",
        "  results = model(image)\n",
        "\n",
        "    # Extract predictions\n",
        "  pred_boxes = results[0].boxes  # Get the bounding boxes from the first image in the results\n",
        "\n",
        "    # If no predictions were made\n",
        "  if len(pred_boxes) == 0:\n",
        "        print(\"couldn't extract license plate from image\")\n",
        "        return\n",
        "\n",
        "    # Select the bounding box with the highest confidence\n",
        "  best_box = max(pred_boxes, key=lambda box: box.conf.cpu().numpy()[0])\n",
        "\n",
        "  best_box = max(pred_boxes, key=lambda box: box.conf.cpu().numpy()[0])\n",
        "\n",
        "    # Convert coordinates from tensor to numpy array for the best box\n",
        "  xyxy = best_box.xyxy.cpu().numpy()[0]\n",
        "  conf = best_box.conf.cpu().numpy()[0]\n",
        "  cls = int(best_box.cls.cpu().numpy()[0])\n",
        "  label = f'{model.names[cls]} {conf:.2f}'\n",
        "    #get the bounding box coordinates\n",
        "  x_min, y_min, x_max, y_max = map(int, xyxy)\n",
        "   # Crop the image using the bounding box coordinates\n",
        "  cropped_image = image[y_min:y_max, x_min:x_max]\n",
        "\n",
        "  image_filename = os.path.basename(img_path)\n",
        "  cropped_path = \"/content/drive/MyDrive/deep-text-recognition-benchmark/cropped_images\"\n",
        "  cropped_image_path=os.path.join(cropped_path,image_filename)\n",
        "  cv2_imshow(cropped_image)\n",
        "  cv2.imwrite(cropped_image_path, cropped_image)\n",
        "  #the following line executes the demo_for_singel_pred.py file wich makes the easy ocr model prediction based on the cropped image\n",
        "  !python /content/drive/MyDrive/deep-text-recognition-benchmark/demo_for_singel_pred.py --Transformation TPS --FeatureExtraction ResNet --SequenceModeling BiLSTM --Prediction Attn --image_folder /content/drive/MyDrive/deep-text-recognition-benchmark/cropped_images --saved_model /content/drive/MyDrive/deep-text-recognition-benchmark/saved_models/best_model/best_norm_ED.pth\n",
        "\n",
        "  if os.path.exists(cropped_image_path):\n",
        "      os.remove(cropped_image_path)\n",
        "  else:\n",
        "      print(f'{cropped_image_path} not found, could not delete.')\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "cfcAGBtHaFdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "predict_one function takes an image path and prints the prediction"
      ],
      "metadata": {
        "id": "wF_YClBi43ND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lmdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op4mR6UBdo7T",
        "outputId": "a5dc3091-683f-48a1-b5fc-6f81a6f0a48a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lmdb\n",
            "  Downloading lmdb-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Downloading lmdb-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lmdb\n",
            "Successfully installed lmdb-1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_one(\"/content/images/img_17.jpg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "ZHaI92uedUjc",
        "outputId": "25c4a019-8f13-4be0-96e1-bfae55a5c916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 576x640 1 matricule, 198.7ms\n",
            "Speed: 10.9ms preprocess, 198.7ms inference, 1.9ms postprocess per image at shape (1, 3, 576, 640)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=90x32>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/deep-text-recognition-benchmark/demo_for_singel_pred.py:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(opt.saved_model, map_location=device))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "--------------------------------------------------------------------------------\n",
            "image                    \tpredicted_labels         \tconfidence score\n",
            "--------------------------------------------------------------------------------\n",
            "img_17.jpg               \t3510AB07                 \t0.9771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_many(img_folder_path):\n",
        "  paths = []\n",
        "  for dirpath,dirnames,filenames in os.walk(img_folder_path):\n",
        "    for filename in filenames:\n",
        "      image_path=os.path.join(dirpath,filename)\n",
        "      paths+=[image_path]\n",
        "\n",
        "  model = YOLO('/content/drive/MyDrive/TrainYolov8CustomDataset/best (1).pt')\n",
        "\n",
        "\n",
        "\n",
        "# Variables to track images with no predictions\n",
        "  no_prediction_count = 0\n",
        "\n",
        "  # Process each image\n",
        "  for path in paths:\n",
        "      # Load an image\n",
        "      image = cv2.imread(path)\n",
        "\n",
        "      # Check if the image was loaded successfully\n",
        "      if image is None:\n",
        "          print(f\"Error: Could not load image at path: {path}\")\n",
        "          continue\n",
        "\n",
        "      # Perform inference\n",
        "      results = model(image)\n",
        "\n",
        "      # Extract predictions\n",
        "      pred_boxes = results[0].boxes  # Get the bounding boxes from the first image in the results\n",
        "\n",
        "      # If no predictions were made\n",
        "      if len(pred_boxes) == 0:\n",
        "          no_prediction_count += 1\n",
        "          continue\n",
        "\n",
        "      # Select the bounding box with the highest confidence\n",
        "      best_box = max(pred_boxes, key=lambda box: box.conf.cpu().numpy()[0])\n",
        "\n",
        "      # Convert coordinates from tensor to numpy array for the best box\n",
        "      xyxy = best_box.xyxy.cpu().numpy()[0]\n",
        "      conf = best_box.conf.cpu().numpy()[0]\n",
        "      cls = int(best_box.cls.cpu().numpy()[0])\n",
        "      label = f'{model.names[cls]} {conf:.2f}'\n",
        "      #get the bounding box coordinates\n",
        "      x_min, y_min, x_max, y_max = map(int, xyxy)\n",
        "    # Crop the image using the bounding box coordinates\n",
        "      cropped_image = image[y_min:y_max, x_min:x_max]\n",
        "      # Optionally, save or display the cropped image\n",
        "      image_filename = os.path.basename(path)\n",
        "      #cv2_imshow(cropped_image)\n",
        "      cropped_image_path=os.path.join(\"/content/drive/MyDrive/TrainYolov8CustomDataset/cropped_predictions_yolo\",image_filename)\n",
        "      cv2.imwrite(cropped_image_path, cropped_image)\n",
        "      #the next line of code runs demo.py responsable of easy ocr model prediction for text recognition\n",
        "  !python /content/drive/MyDrive/deep-text-recognition-benchmark/demo.py --Transformation TPS --FeatureExtraction ResNet --SequenceModeling BiLSTM --Prediction Attn --image_folder /content/drive/MyDrive/TrainYolov8CustomDataset/cropped_predictions_yolo --saved_model /content/drive/MyDrive/deep-text-recognition-benchmark/saved_models/best_model/best_norm_ED.pth\n",
        "  return\n"
      ],
      "metadata": {
        "id": "rhF-VGL3lw9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The predict_many function takes as input an images folder path then it prints the predictions and saves them in log_demo_results.txt"
      ],
      "metadata": {
        "id": "jHS3JXyFdu4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_many(\"/content/drive/MyDrive/TrainYolov8CustomDataset/data/images/test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZHs0RsTALn8",
        "outputId": "6f1ce9f8-4709-4200-f20a-27c05c8cb9ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 576x640 1 matricule, 205.7ms\n",
            "Speed: 4.2ms preprocess, 205.7ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
            "\n",
            "0: 480x640 1 matricule, 432.1ms\n",
            "Speed: 5.9ms preprocess, 432.1ms inference, 7.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 1 matricule, 425.5ms\n",
            "Speed: 5.4ms preprocess, 425.5ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 608x640 1 matricule, 223.7ms\n",
            "Speed: 6.9ms preprocess, 223.7ms inference, 1.0ms postprocess per image at shape (1, 3, 608, 640)\n",
            "\n",
            "0: 480x640 1 matricule, 178.9ms\n",
            "Speed: 4.0ms preprocess, 178.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 164.3ms\n",
            "Speed: 5.0ms preprocess, 164.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 512x640 1 matricule, 175.9ms\n",
            "Speed: 5.0ms preprocess, 175.9ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "0: 448x640 1 matricule, 206.1ms\n",
            "Speed: 5.0ms preprocess, 206.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 320x640 1 matricule, 148.3ms\n",
            "Speed: 5.1ms preprocess, 148.3ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 203.2ms\n",
            "Speed: 4.8ms preprocess, 203.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 1 matricule, 232.0ms\n",
            "Speed: 4.7ms preprocess, 232.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 608x640 1 matricule, 363.0ms\n",
            "Speed: 6.5ms preprocess, 363.0ms inference, 1.2ms postprocess per image at shape (1, 3, 608, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 261.1ms\n",
            "Speed: 4.9ms preprocess, 261.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 544x640 1 matricule, 293.8ms\n",
            "Speed: 6.1ms preprocess, 293.8ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
            "\n",
            "0: 544x640 1 matricule, 311.2ms\n",
            "Speed: 5.3ms preprocess, 311.2ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 257.7ms\n",
            "Speed: 6.9ms preprocess, 257.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x512 1 matricule, 338.7ms\n",
            "Speed: 13.1ms preprocess, 338.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "0: 480x640 1 matricule, 283.7ms\n",
            "Speed: 4.2ms preprocess, 283.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x608 1 matricule, 316.4ms\n",
            "Speed: 8.6ms preprocess, 316.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 608)\n",
            "\n",
            "0: 640x384 1 matricule, 236.9ms\n",
            "Speed: 4.0ms preprocess, 236.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "0: 480x640 1 matricule, 274.2ms\n",
            "Speed: 3.7ms preprocess, 274.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 384x640 1 matricule, 219.5ms\n",
            "Speed: 4.1ms preprocess, 219.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 301.0ms\n",
            "Speed: 4.3ms preprocess, 301.0ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 608x640 1 matricule, 351.3ms\n",
            "Speed: 9.5ms preprocess, 351.3ms inference, 1.3ms postprocess per image at shape (1, 3, 608, 640)\n",
            "\n",
            "0: 480x640 3 matricules, 263.9ms\n",
            "Speed: 4.1ms preprocess, 263.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 matricule, 231.2ms\n",
            "Speed: 4.8ms preprocess, 231.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 1 matricule, 158.9ms\n",
            "Speed: 5.7ms preprocess, 158.9ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 187.9ms\n",
            "Speed: 5.1ms preprocess, 187.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 256x640 1 matricule, 101.7ms\n",
            "Speed: 3.8ms preprocess, 101.7ms inference, 0.8ms postprocess per image at shape (1, 3, 256, 640)\n",
            "\n",
            "0: 480x640 1 matricule, 165.6ms\n",
            "Speed: 5.1ms preprocess, 165.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 162.8ms\n",
            "Speed: 5.1ms preprocess, 162.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 1 matricule, 183.2ms\n",
            "Speed: 4.1ms preprocess, 183.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 416x640 1 matricule, 141.0ms\n",
            "Speed: 4.8ms preprocess, 141.0ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "0: 576x640 1 matricule, 212.5ms\n",
            "Speed: 4.5ms preprocess, 212.5ms inference, 1.0ms postprocess per image at shape (1, 3, 576, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 165.7ms\n",
            "Speed: 5.3ms preprocess, 165.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 544x640 1 matricule, 203.0ms\n",
            "Speed: 5.3ms preprocess, 203.0ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 640)\n",
            "\n",
            "0: 416x640 1 matricule, 149.3ms\n",
            "Speed: 3.4ms preprocess, 149.3ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "0: 384x640 1 matricule, 141.8ms\n",
            "Speed: 4.6ms preprocess, 141.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 163.4ms\n",
            "Speed: 6.4ms preprocess, 163.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 2 matricules, 178.1ms\n",
            "Speed: 5.1ms preprocess, 178.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 matricule, 177.9ms\n",
            "Speed: 4.7ms preprocess, 177.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 1 matricule, 163.8ms\n",
            "Speed: 4.0ms preprocess, 163.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x640 2 matricules, 214.4ms\n",
            "Speed: 6.0ms preprocess, 214.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 167.6ms\n",
            "Speed: 4.5ms preprocess, 167.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x640 1 matricule, 244.3ms\n",
            "Speed: 4.4ms preprocess, 244.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 480x640 1 matricule, 162.1ms\n",
            "Speed: 5.0ms preprocess, 162.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 180.0ms\n",
            "Speed: 5.1ms preprocess, 180.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 576x640 1 matricule, 192.1ms\n",
            "Speed: 5.5ms preprocess, 192.1ms inference, 1.0ms postprocess per image at shape (1, 3, 576, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 187.1ms\n",
            "Speed: 4.9ms preprocess, 187.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x640 1 matricule, 219.3ms\n",
            "Speed: 4.3ms preprocess, 219.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 416x640 1 matricule, 140.6ms\n",
            "Speed: 5.3ms preprocess, 140.6ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 177.9ms\n",
            "Speed: 4.9ms preprocess, 177.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 1 matricule, 165.2ms\n",
            "Speed: 5.2ms preprocess, 165.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 416x640 1 matricule, 165.8ms\n",
            "Speed: 4.4ms preprocess, 165.8ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "0: 416x640 1 matricule, 145.6ms\n",
            "Speed: 6.6ms preprocess, 145.6ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 164.9ms\n",
            "Speed: 5.1ms preprocess, 164.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 matricule, 179.0ms\n",
            "Speed: 4.9ms preprocess, 179.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 512x640 1 matricule, 170.3ms\n",
            "Speed: 5.0ms preprocess, 170.3ms inference, 0.9ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "0: 576x640 1 matricule, 219.1ms\n",
            "Speed: 4.4ms preprocess, 219.1ms inference, 1.0ms postprocess per image at shape (1, 3, 576, 640)\n",
            "\n",
            "0: 480x640 1 matricule, 163.4ms\n",
            "Speed: 4.7ms preprocess, 163.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 166.6ms\n",
            "Speed: 3.9ms preprocess, 166.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 1 matricule, 173.4ms\n",
            "Speed: 5.4ms preprocess, 173.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x640 1 matricule, 236.6ms\n",
            "Speed: 6.0ms preprocess, 236.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 165.6ms\n",
            "Speed: 6.3ms preprocess, 165.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 608x640 1 matricule, 214.9ms\n",
            "Speed: 4.5ms preprocess, 214.9ms inference, 0.9ms postprocess per image at shape (1, 3, 608, 640)\n",
            "\n",
            "0: 640x384 1 matricule, 139.9ms\n",
            "Speed: 4.6ms preprocess, 139.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "0: 640x480 1 matricule, 185.9ms\n",
            "Speed: 4.1ms preprocess, 185.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x640 1 matricule, 215.0ms\n",
            "Speed: 6.1ms preprocess, 215.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x384 1 matricule, 133.7ms\n",
            "Speed: 2.7ms preprocess, 133.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "0: 480x640 1 matricule, 192.6ms\n",
            "Speed: 6.3ms preprocess, 192.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 209.0ms\n",
            "Speed: 5.1ms preprocess, 209.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 matricule, 265.2ms\n",
            "Speed: 5.8ms preprocess, 265.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 matricule, 256.5ms\n",
            "Speed: 5.6ms preprocess, 256.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 1 matricule, 286.8ms\n",
            "Speed: 4.7ms preprocess, 286.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 255.7ms\n",
            "Speed: 5.9ms preprocess, 255.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 384x640 1 matricule, 229.1ms\n",
            "Speed: 4.0ms preprocess, 229.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 266.6ms\n",
            "Speed: 7.3ms preprocess, 266.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x384 1 matricule, 236.0ms\n",
            "Speed: 4.0ms preprocess, 236.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
            "\n",
            "0: 640x480 1 matricule, 273.9ms\n",
            "Speed: 8.6ms preprocess, 273.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 matricule, 269.1ms\n",
            "Speed: 4.0ms preprocess, 269.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 448x640 1 matricule, 247.1ms\n",
            "Speed: 3.3ms preprocess, 247.1ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 268.8ms\n",
            "Speed: 9.9ms preprocess, 268.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x576 1 matricule, 347.5ms\n",
            "Speed: 6.8ms preprocess, 347.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 576)\n",
            "\n",
            "0: 480x640 1 matricule, 166.8ms\n",
            "Speed: 5.5ms preprocess, 166.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 182.7ms\n",
            "Speed: 6.3ms preprocess, 182.7ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 1 matricule, 166.2ms\n",
            "Speed: 5.8ms preprocess, 166.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 161.9ms\n",
            "Speed: 5.7ms preprocess, 161.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 matricule, 190.5ms\n",
            "Speed: 5.6ms preprocess, 190.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 1 matricule, 166.5ms\n",
            "Speed: 5.8ms preprocess, 166.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 matricule, 193.5ms\n",
            "Speed: 6.1ms preprocess, 193.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 164.0ms\n",
            "Speed: 5.9ms preprocess, 164.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 matricule, 162.4ms\n",
            "Speed: 5.9ms preprocess, 162.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 matricule, 175.4ms\n",
            "Speed: 6.6ms preprocess, 175.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 matricule, 176.1ms\n",
            "Speed: 6.2ms preprocess, 176.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 1 matricule, 156.8ms\n",
            "Speed: 6.0ms preprocess, 156.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 176.1ms\n",
            "Speed: 5.6ms preprocess, 176.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 matricule, 178.0ms\n",
            "Speed: 6.4ms preprocess, 178.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 1 matricule, 163.9ms\n",
            "Speed: 6.0ms preprocess, 163.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x480 1 matricule, 163.4ms\n",
            "Speed: 6.4ms preprocess, 163.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 matricule, 168.9ms\n",
            "Speed: 5.1ms preprocess, 168.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "model input parameters 32 100 20 1 512 256 38 25 TPS ResNet BiLSTM Attn\n",
            "loading pretrained model from /content/drive/MyDrive/deep-text-recognition-benchmark/saved_models/best_model/best_norm_ED.pth\n",
            "/content/drive/MyDrive/deep-text-recognition-benchmark/demo.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(opt.saved_model, map_location=device))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "img_1.jpg                \t8630AB06                 \t0.9973\n",
            "img_2.jpg                \t5896AA08                 \t0.9679\n",
            "img_3.jpg                \t3395AB07                 \t0.9874\n",
            "img_4.jpg                \t4710AA03                 \t0.9806\n",
            "img_5.jpg                \t7145AL00                 \t0.9338\n",
            "img_6.jpg                \t0722AB03                 \t0.9921\n",
            "img_7.jpg                \t5601AA08                 \t0.9882\n",
            "img_8.jpg                \t5688AM00                 \t0.9659\n",
            "img_9.jpg                \t7438AB08                 \t0.9970\n",
            "img_10.jpg               \t5115AM00                 \t0.9924\n",
            "img_12.jpg               \t5008AR00                 \t0.8825\n",
            "img_13.jpg               \t1112AB06                 \t0.9926\n",
            "img_16.jpg               \t1806AY00                 \t0.9900\n",
            "img_17.jpg               \t3726AN00                 \t0.9638\n",
            "img_18.jpg               \t0505BA00                 \t0.9841\n",
            "img_19.jpg               \t0195A006                 \t0.0774\n",
            "img_20.jpg               \t6606AB07                 \t0.8442\n",
            "img_22.jpg               \t0714AZ08                 \t0.3164\n",
            "img_24.jpg               \t3775AU00                 \t0.9708\n",
            "img_25.jpg               \t4914AB06                 \t0.9952\n",
            "img_26.jpg               \t5949AB08                 \t0.9480\n",
            "img_27.jpg               \t5366AB08                 \t0.9739\n",
            "img_28.jpg               \t3791AB06                 \t0.9672\n",
            "img_29.jpg               \t8485AB06                 \t0.9927\n",
            "img_30.jpg               \t7133AB07                 \t0.9881\n",
            "img_31.jpg               \t8550AS00                 \t0.9926\n",
            "img_32.jpg               \t4111AA01                 \t0.9499\n",
            "img_33.jpg               \t1406BA00                 \t0.9855\n",
            "img_34.jpg               \t5700AB08                 \t0.8164\n",
            "img_35.jpg               \t0908AU00                 \t0.9887\n",
            "img_37.jpg               \t6206AA07                 \t0.9901\n",
            "img_38.jpg               \t7080AY00                 \t0.9952\n",
            "img_40.jpg               \t3337AV00                 \t0.9882\n",
            "img_41.jpg               \t5391AC06                 \t0.9925\n",
            "img_43.jpg               \t4766AA06                 \t0.9731\n",
            "img_45.jpg               \t6361AV00                 \t0.9219\n",
            "img_46.jpg               \t0081AZ00                 \t0.9508\n",
            "img_47.jpg               \t5853AR08                 \t0.5619\n",
            "img_48.jpg               \t9890AX00                 \t0.9938\n",
            "img_49.jpg               \t9150AB06                 \t0.9899\n",
            "img_50.jpg               \t1418AY00                 \t0.9956\n",
            "img_51.jpg               \t9322AA07                 \t0.9786\n",
            "img_52.jpg               \t4336AX00                 \t0.9968\n",
            "img_53.jpg               \t2988AX00                 \t0.9962\n",
            "img_55.jpg               \t3540AB08                 \t0.9527\n",
            "img_56.jpg               \t1901AA05                 \t0.9948\n",
            "img_57.jpg               \t7734AB06                 \t0.9932\n",
            "img_58.jpg               \t2389AB07                 \t0.9847\n",
            "img_59.jpg               \t0962AB03                 \t0.9892\n",
            "img_60.jpg               \t9189AA03                 \t0.9951\n",
            "img_61.jpg               \t3221AM00                 \t0.9942\n",
            "img_63.jpg               \t7180AG00                 \t0.9935\n",
            "img_65.jpg               \t8348AA02                 \t0.9568\n",
            "img_66.jpg               \t5546AB06                 \t0.9524\n",
            "img_67.jpg               \t1949AA01                 \t0.9747\n",
            "img_68.jpg               \t8520AN00                 \t0.9979\n",
            "img_69.jpg               \t5588AY00                 \t0.9907\n",
            "img_70.jpg               \t1693AB06                 \t0.9948\n",
            "img_72.jpg               \t0141AC06                 \t0.9858\n",
            "img_73.jpg               \t5600AX00                 \t0.9909\n",
            "img_74.jpg               \t4882AA12                 \t0.9898\n",
            "img_75.jpg               \t6084AY00                 \t0.9930\n",
            "img_76.jpg               \t2659AA09                 \t0.9838\n",
            "img_77.jpg               \t4153AB07                 \t0.7336\n",
            "img_78.jpg               \t8856AA02                 \t0.9612\n",
            "img_80.jpg               \t1114AA01                 \t0.9775\n",
            "img_81.jpg               \t7365AA02                 \t0.9256\n",
            "img_82.jpg               \t8594AA00                 \t0.2246\n",
            "img_83.jpg               \t6533AA07                 \t0.9895\n",
            "img_84.jpg               \t3998AA06                 \t0.9915\n",
            "img_85.jpg               \t6095AV00                 \t0.9934\n",
            "img_86.jpg               \t1261AA10                 \t0.9941\n",
            "img_87.jpg               \t8271AB06                 \t0.7071\n",
            "img_88.jpg               \t9824AA07                 \t0.9901\n",
            "img_89.jpg               \t4250AA06                 \t0.9458\n",
            "img_90.jpg               \t7121AA12                 \t0.9889\n",
            "img_91.jpg               \t0714AZ08                 \t0.9346\n",
            "img_92.jpg               \t2339AA09                 \t0.9906\n",
            "img_94.jpg               \t0540AX00                 \t0.9777\n",
            "img_96.jpg               \t1964AB08                 \t0.9973\n",
            "img_97.jpg               \t2734AB07                 \t0.9783\n",
            "img_98.jpg               \t9910AA08                 \t0.9897\n",
            "img_99.jpg               \t1480AV00                 \t0.9967\n",
            "img_100.jpg              \t7812AA02                 \t0.9918\n",
            "img_101.jpg              \t8944AA06                 \t0.9674\n",
            "img_102.jpg              \t6421AA03                 \t0.9892\n",
            "img_103.jpg              \t0099AC08                 \t0.9978\n",
            "img_104.jpg              \t5051AY00                 \t0.9963\n",
            "img_106.jpg              \t1277AY00                 \t0.9625\n",
            "img_107.jpg              \t5235AB07                 \t0.9918\n",
            "img_109.jpg              \t6602AB07                 \t0.9923\n",
            "img_111.jpg              \t3085AA12                 \t0.7827\n",
            "img_112.jpg              \t6563AR12                 \t0.1745\n",
            "img_113.jpg              \t9838AA08                 \t0.5818\n",
            "img_114.jpg              \t8183AA07                 \t0.9906\n",
            "img_115.jpg              \t6823AA12                 \t0.9916\n",
            "img_116.jpg              \t2450AX00                 \t0.9945\n",
            "img_117.jpg              \t4818AX00                 \t0.9946\n",
            "img_118.jpg              \t3664AB06                 \t0.9845\n",
            "img_119.jpg              \t5070AP00                 \t0.9804\n",
            "img_120.jpg              \t3510AB07                 \t0.9771\n",
            "img_121.jpg              \t9687AB08                 \t0.9933\n",
            "img_122.jpg              \t6757AU00                 \t0.9968\n",
            "img_123.jpg              \t8519AA03                 \t0.9710\n",
            "img_124.jpg              \t5215AA12                 \t0.9934\n",
            "img_125.jpg              \t4618AY00                 \t0.8361\n",
            "img_126.jpg              \t1580AV00                 \t0.9913\n",
            "img_128.jpg              \t8900AA07                 \t0.9926\n",
            "img_129.jpg              \t1966AB08                 \t0.9821\n",
            "img_130.jpg              \t2534AB06                 \t0.9602\n",
            "img_131.jpg              \t4922AB07                 \t0.9703\n",
            "img_133.jpg              \t3569AA04                 \t0.9818\n",
            "img_134.jpg              \t7427AA06                 \t0.5755\n",
            "img_136.jpg              \t3673AB06                 \t0.9963\n",
            "img_137.jpg              \t3079AM00                 \t0.9913\n",
            "img_139.jpg              \t1700AA05                 \t0.9938\n",
            "img_140.jpg              \t1417AX00                 \t0.9891\n",
            "img_141.jpg              \t5313AR00                 \t0.9744\n",
            "img_142.jpg              \t5183AA12                 \t0.9888\n",
            "img_143.jpg              \t8896AA07                 \t0.9787\n",
            "img_144.jpg              \t1709AA05                 \t0.9919\n",
            "img_145.jpg              \t5189AB07                 \t0.9858\n",
            "img_146.jpg              \t9039AB08                 \t0.9811\n",
            "img_147.jpg              \t0153AB08                 \t0.9726\n",
            "img_148.jpg              \t1874AP00                 \t0.9864\n",
            "img_149.jpg              \t1428AL00                 \t0.9880\n",
            "img_150.jpg              \t9227AA08                 \t0.9742\n",
            "img_151.jpg              \t1579AA12                 \t0.9953\n",
            "img_152.jpg              \t7639AG00                 \t0.9964\n",
            "img_153.jpg              \t7471AR00                 \t0.9505\n",
            "img_154.jpg              \t3959AA05                 \t0.9367\n",
            "img_156.jpg              \t6831AB08                 \t0.9727\n",
            "img_157.jpg              \t8594AA07                 \t0.9900\n",
            "img_158.jpg              \t0715AB07                 \t0.9944\n",
            "img_162.jpg              \t5220AA03                 \t0.9912\n",
            "img_163.jpg              \t7819AB08                 \t0.9865\n",
            "img_164.jpg              \t8146AB08                 \t0.9867\n",
            "img_165.jpg              \t6827AX00                 \t0.9814\n",
            "img_166.jpg              \t0368AS00                 \t0.9762\n",
            "img_168.jpg              \t2650AZ00                 \t0.9730\n",
            "img_169.jpg              \t8810AB08                 \t0.9894\n",
            "img_172.jpg              \t7526AB06                 \t0.9945\n",
            "img_174.jpg              \t3807AA08                 \t0.9719\n",
            "img_175.jpg              \t1833AA12                 \t0.9905\n",
            "img_176.jpg              \t5178AA07                 \t0.9896\n",
            "img_179.jpg              \t5778AR00                 \t0.9735\n",
            "img_180.jpg              \t8303AA03                 \t0.9937\n",
            "img_181.jpg              \t5003AA03                 \t0.9981\n",
            "img_182.jpg              \t0476AX00                 \t0.9933\n",
            "img_183.jpg              \t2320AY00                 \t0.9864\n",
            "img_184.jpg              \t0478AB08                 \t0.9962\n",
            "img_185.jpg              \t3699AR06                 \t0.9788\n",
            "img_186.jpg              \t2025AB06                 \t0.9260\n",
            "img_188.jpg              \t6956AU00                 \t0.9880\n",
            "img_189.jpg              \t0732AA05                 \t0.9837\n",
            "img_190.jpg              \t2119AA09                 \t0.9960\n",
            "img_191.jpg              \t5215AA12                 \t0.9941\n",
            "img_193.jpg              \t8839AY00                 \t0.9474\n",
            "img_194.jpg              \t0294AB07                 \t0.9665\n",
            "img_195.jpg              \t7037AA07                 \t0.9887\n",
            "img_196.jpg              \t7363AX00                 \t0.9689\n",
            "img_197.jpg              \t2928BA00                 \t0.9835\n",
            "img_198.jpg              \t1604AB06                 \t0.9960\n",
            "img_199.jpg              \t1987AA09                 \t0.9940\n",
            "img_200.jpg              \t9174AZ00                 \t0.9807\n",
            "img_201.jpg              \t5654AA08                 \t0.9155\n",
            "img_202.jpg              \t5620AB06                 \t0.9348\n",
            "img_203.jpg              \t0427AY00                 \t0.9850\n",
            "img_204.jpg              \t1206AB06                 \t0.9734\n",
            "img_206.jpg              \t6353AA08                 \t0.9818\n",
            "img_207.jpg              \t6934AA02                 \t0.9925\n",
            "img_208.jpg              \t4934AA12                 \t0.9927\n",
            "img_209.jpg              \t2539AA12                 \t0.9958\n",
            "img_210.jpg              \t7923AN00                 \t0.9963\n",
            "img_211.jpg              \t2336AA05                 \t0.9934\n",
            "img_212.jpg              \t0561AC08                 \t0.9900\n",
            "img_213.jpg              \t5521AA05                 \t0.9919\n",
            "img_214.jpg              \t6425AA06                 \t0.9914\n",
            "img_215.jpg              \t4975AB06                 \t0.9898\n",
            "img_216.jpg              \t4139AB08                 \t0.9938\n",
            "img_217.jpg              \t4890AY00                 \t0.9969\n",
            "img_221.jpg              \t9530AA07                 \t0.9930\n",
            "img_226.jpg              \t2336AA05                 \t0.9942\n",
            "img_227.jpg              \t9688AR10                 \t0.7846\n",
            "img_228.jpg              \t2116AX80                 \t0.9029\n",
            "img_229.jpg              \t3850AA08                 \t0.9108\n",
            "img_232.jpg              \t8978AY00                 \t0.9928\n",
            "img_233.jpg              \t7870AA06                 \t0.9894\n",
            "img_234.jpg              \t4875AB06                 \t0.9860\n",
            "img_235.jpg              \t2352AA09                 \t0.9733\n",
            "img_236.jpg              \t6699AG00                 \t0.9882\n",
            "img_237.jpg              \t2619AA00                 \t0.9959\n",
            "img_238.jpg              \t7139AA07                 \t0.9802\n",
            "img_240.jpg              \t5438AA06                 \t0.9922\n",
            "img_243.jpg              \t5957AM03                 \t0.9804\n",
            "img_244.jpg              \t1347AB06                 \t0.9878\n",
            "img_245.jpg              \t1901AA05                 \t0.7715\n",
            "img_246.jpg              \t7360AR00                 \t0.9923\n",
            "img_248.jpg              \t6602AB07                 \t0.9909\n",
            "img_249.jpg              \t5780AB08                 \t0.9881\n",
            "img_250.jpg              \t3256AY00                 \t0.9917\n",
            "img_252.jpg              \t3614AR00                 \t0.9928\n",
            "img_253.jpg              \t9476AR00                 \t0.9902\n",
            "img_256.jpg              \t2077AA05                 \t0.9252\n",
            "img_257.jpg              \t1085AC08                 \t0.6392\n",
            "img_258.jpg              \t3089AX00                 \t0.1900\n",
            "img_259.jpg              \t1517AA09                 \t0.9920\n",
            "img_260.jpg              \t7338AA07                 \t0.9800\n",
            "img_261.jpg              \t2828AU00                 \t0.4934\n",
            "img_262.jpg              \t8411AA08                 \t0.9643\n",
            "img_263.jpg              \t1478AU00                 \t0.9979\n",
            "img_264.jpg              \t8784AA08                 \t0.8608\n",
            "img_265.jpg              \t0914BA00                 \t0.9886\n",
            "img_266.jpg              \t6831AB08                 \t0.9932\n",
            "img_267.jpg              \t9617AG00                 \t0.9836\n",
            "img_269.jpg              \t7110AM00                 \t0.9859\n",
            "img_270.jpg              \t6878AU00                 \t0.9811\n",
            "img_271.jpg              \t9984AA06                 \t0.5670\n",
            "img_272.jpg              \t8904AA07                 \t0.9849\n",
            "img_273.jpg              \t7098AL00                 \t0.9798\n",
            "img_274.jpg              \t0099AC08                 \t0.9884\n",
            "img_277.jpg              \t0624AB06                 \t0.9434\n",
            "img_278.jpg              \t1693AB06                 \t0.9842\n",
            "img_279.jpg              \t0141AC06                 \t0.9809\n",
            "img_280.jpg              \t0614AB08                 \t0.9944\n",
            "img_281.jpg              \t5373AV00                 \t0.9880\n",
            "img_283.jpg              \t1957BA00                 \t0.9655\n",
            "img_284.jpg              \t5649AA08                 \t0.9859\n",
            "img_285.jpg              \t1508AA09                 \t0.9943\n",
            "img_286.jpg              \t0848AV00                 \t0.9818\n",
            "img_288.jpg              \t3244AX00                 \t0.9808\n",
            "img_289.jpg              \t1776AX00                 \t0.9940\n",
            "img_290.jpg              \t3292AC06                 \t0.8341\n",
            "img_291.jpg              \t4296AG00                 \t0.9818\n",
            "img_292.jpg              \t7160AS00                 \t0.9896\n",
            "img_293.jpg              \t9879AP00                 \t0.9946\n",
            "img_294.jpg              \t2950AB08                 \t0.9949\n",
            "img_295.jpg              \t1365AA09                 \t0.9881\n",
            "img_296.jpg              \t9942AL00                 \t0.9935\n",
            "img_297.jpg              \t6025AA12                 \t0.9904\n",
            "img_298.jpg              \t6454AB07                 \t0.9782\n",
            "img_299.jpg              \t5555AA07                 \t0.9772\n",
            "img_302.jpg              \t7145AA12                 \t0.9569\n",
            "img_304.jpg              \t0225AC06                 \t0.9931\n",
            "img_306.jpg              \t3189AB07                 \t0.9882\n",
            "img_308.jpg              \t4602AA12                 \t0.9938\n",
            "img_309.jpg              \t8798AP00                 \t0.9957\n",
            "img_312.jpg              \t6281AB07                 \t0.9840\n",
            "img_313.jpg              \t3673AB06                 \t0.9963\n",
            "img_314.jpg              \t9025AX00                 \t0.9765\n",
            "img_316.jpg              \t0765AB06                 \t0.9946\n",
            "img_317.jpg              \t0701AP00                 \t0.5910\n",
            "img_318.jpg              \t0024AU00                 \t0.9921\n",
            "img_319.jpg              \t2429AA03                 \t0.9940\n",
            "img_320.jpg              \t7309AG00                 \t0.9787\n",
            "img_321.jpg              \t0445AS00                 \t0.6247\n",
            "img_322.jpg              \t4118AU00                 \t0.9926\n",
            "img_324.jpg              \t5370AK00                 \t0.9882\n",
            "img_325.jpg              \t8591AA06                 \t0.9896\n",
            "img_327.jpg              \t2760AB06                 \t0.9870\n",
            "img_328.jpg              \t5917AA09                 \t0.9741\n",
            "img_329.jpg              \t7982AA06                 \t0.9937\n",
            "img_330.jpg              \t5022AA03                 \t0.9847\n",
            "img_331.jpg              \t1814AY00                 \t0.9961\n",
            "img_332.jpg              \t5263AA03                 \t0.9920\n",
            "img_333.jpg              \t1833AA03                 \t0.9954\n",
            "img_334.jpg              \t3673AB06                 \t0.9955\n",
            "img_335.jpg              \t0483AB08                 \t0.9968\n",
            "img_336.jpg              \t3463AA02                 \t0.9905\n",
            "img_337.jpg              \t6584AB07                 \t0.9823\n",
            "img_338.jpg              \t1975AA09                 \t0.9902\n",
            "img_339.jpg              \t8457AK00                 \t0.9873\n",
            "img_340.jpg              \t7664AA08                 \t0.9937\n",
            "img_341.jpg              \t3292AU00                 \t0.9855\n",
            "img_342.jpg              \t0078AZ00                 \t0.9606\n",
            "img_343.jpg              \t3578AB06                 \t0.9900\n",
            "img_344.jpg              \t7126AL00                 \t0.9905\n",
            "img_345.jpg              \t6930AE00                 \t0.9867\n",
            "img_346.jpg              \t4052AA05                 \t0.9912\n",
            "img_347.jpg              \t9770AA06                 \t0.9897\n",
            "img_348.jpg              \t8862AB06                 \t0.9834\n",
            "img_349.jpg              \t6074AA02                 \t0.9907\n",
            "img_350.jpg              \t5678AA07                 \t0.9865\n",
            "img_351.jpg              \t2228AA12                 \t0.9901\n",
            "img_352.jpg              \t9639AS00                 \t0.9688\n",
            "img_353.jpg              \t8814AV00                 \t0.9972\n",
            "img_354.jpg              \t2900AA12                 \t0.9949\n",
            "img_355.jpg              \t0940AC08                 \t0.9772\n",
            "img_356.jpg              \t2316AZ00                 \t0.9899\n",
            "img_357.jpg              \t1575AS00                 \t0.9889\n",
            "img_358.jpg              \t4363AB07                 \t0.9935\n",
            "img_359.jpg              \t1347AB06                 \t0.9936\n",
            "img_360.jpg              \t3383AB08                 \t0.9953\n",
            "img_361.jpg              \t4740AA02                 \t0.9712\n",
            "img_362.jpg              \t5586AA12                 \t0.9912\n",
            "img_363.jpg              \t8143AV00                 \t0.9744\n",
            "img_364.jpg              \t7783AB08                 \t0.9590\n",
            "img_365.jpg              \t3074AA00                 \t0.1426\n",
            "img_366.jpg              \t0904AB06                 \t0.9970\n",
            "img_367.jpg              \t3792AA03                 \t0.9885\n",
            "img_368.jpg              \t0004AY00                 \t0.9979\n",
            "img_369.jpg              \t8383AV00                 \t0.5291\n",
            "img_370.jpg              \t1762AB08                 \t0.9919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Converting the text data obtained from easy ocr to csv format\n",
        "import csv\n",
        "f=open(\"/content/log_demo_result.csv\",\"w\")\n",
        "f.close()\n",
        "input_log_file = '/content/log_demo_result.txt'\n",
        "output_csv_file = '/content/log_demo_result.csv'\n",
        "\n",
        "# Function to manually parse each line assuming fixed-width columns\n",
        "def parse_line(line):\n",
        "    # Assuming the columns are at fixed positions based on your example\n",
        "    image_path = line[:25].strip()          # First 25 characters\n",
        "    predicted_labels = line[26:49].strip()  # Next 23 characters\n",
        "    confidence_score = line[50:].strip()    # The rest of the line\n",
        "\n",
        "    return [image_path, predicted_labels, confidence_score]\n",
        "\n",
        "# Read the log file and write to a CSV file\n",
        "with open(input_log_file, 'r') as infile, open(output_csv_file, 'w', newline='') as outfile:\n",
        "    writer = csv.writer(outfile)\n",
        "\n",
        "    # Write the header\n",
        "    writer.writerow(['image_path', 'predicted_labels', 'confidence_score'])\n",
        "\n",
        "    # Process each line in the log file\n",
        "    for line in infile:\n",
        "        parsed_row = parse_line(line)\n",
        "        writer.writerow(parsed_row)\n",
        "\n",
        "print(\"Log file has been converted to CSV.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RulvNTj7JC9U",
        "outputId": "ecad279e-aa92-4234-d72e-190a1f1d55c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log file has been converted to CSV.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the above code converts the predictions in log_demo_result.txt to csv format"
      ],
      "metadata": {
        "id": "C8hq3ta0eTJ5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}