{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KrZo2mxIpB0"
      },
      "source": [
        "# Training the OCR model :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7sXh8_RJYEE"
      },
      "source": [
        "**model selection** :\n",
        "after searching a little bit i found some comparisons between some ocr models and easy ocr seemed to perform better than others espacially in license plates number detection ,and it is as well ,according to some random web page i found ,one of the easiest models to fine tune , so we will start with it and see how things go"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL7fKVSuKRIu"
      },
      "source": [
        "**fine tunning easy ocr**\n",
        ":\n",
        "for this task i will be inspiring from a great [article](https://www.freecodecamp.org/news/how-to-fine-tune-easyocr-with-a-synthetic-dataset/) i found on freecodecamp on how to fine tune easy ocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5xDSb0gU6cD",
        "outputId": "95913c06-a133-4118-edda-3dcfcb951097",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fire\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/88.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire) (2.4.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117030 sha256=c9649ce4f1b98f32f4c86450e17ff34e894768b6ee1c2156eef5ed49c618e948\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "Successfully built fire\n",
            "Installing collected packages: fire\n",
            "Successfully installed fire-0.6.0\n",
            "Collecting lmdb\n",
            "  Downloading lmdb-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Downloading lmdb-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lmdb\n",
            "Successfully installed lmdb-1.5.1\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (8.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "#installing necessary librairies\n",
        "!pip install fire\n",
        "!pip install lmdb\n",
        "!pip install opencv-python\n",
        "!pip install natsort\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**note about the next cell:** you may  skip this cell and just use the  repository on our github.\n",
        "In case you chose to clone the original ripository you will have to replace the original train.py,dataset.py and demo.py files with the modified ones in order to run the training and inference correctly  (you will find them in the rim_ai_competiton/deep-text-recognition-benchmark folder)"
      ],
      "metadata": {
        "id": "VEJNHP3Z5bMW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azHBe0gePku1",
        "outputId": "a8da1819-1a78-4e67-db09-5f58497a96f0",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deep-text-recognition-benchmark'...\n",
            "remote: Enumerating objects: 499, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 499 (delta 0), reused 1 (delta 0), pack-reused 495 (from 1)\u001b[K\n",
            "Receiving objects: 100% (499/499), 3.07 MiB | 13.11 MiB/s, done.\n",
            "Resolving deltas: 100% (301/301), done.\n"
          ]
        }
      ],
      "source": [
        "#cloning the git repository  to help us run the fine tuning\n",
        "\n",
        "#!git clone https://github.com/clovaai/deep-text-recognition-benchmark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing the labels for the model training\n",
        "import pandas as pd\n",
        "labels_csv=pd.read_csv(\"/content/drive/MyDrive/rim_ai_competition/train_labels.csv\")#path to the train-labels.csv file provided in the competiton data\n",
        "labels=open(\"/content/drive/MyDrive/deep-text-recognition-benchmark/labels.txt\",'w')#creating a new labels.txt file to contain the labels\n",
        "text=\"\"\n",
        "for i in range(204):\n",
        "\n",
        "  text=text+f\"{labels_csv.iloc[i]['img_id']}.jpg\\t{labels_csv.iloc[i]['plate_number']} \\n\" #writing the labels in  the txt file\n",
        "\n",
        "labels.write(text)\n",
        "labels.close()\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfxOW2GREqfY",
        "outputId": "2d9d5b87-63e3-4282-bd21-f02e7837b902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img_1.jpg\t8630AB06 \n",
            "img_10.jpg\t5115AM00 \n",
            "img_100.jpg\t7812AA02 \n",
            "img_102.jpg\t6421AA03 \n",
            "img_103.jpg\t0099AC08 \n",
            "img_106.jpg\t1277AY00 \n",
            "img_107.jpg\t5235AB07 \n",
            "img_109.jpg\t6602AB07 \n",
            "img_111.jpg\t3085AA12 \n",
            "img_112.jpg\t6563AR12 \n",
            "img_115.jpg\t6823AA12 \n",
            "img_118.jpg\t3664AB06 \n",
            "img_119.jpg\t5070AP00 \n",
            "img_12.jpg\t5008AR00 \n",
            "img_120.jpg\t3510AB07 \n",
            "img_123.jpg\t8519AA03 \n",
            "img_126.jpg\t1580AV00 \n",
            "img_128.jpg\t8900AA07 \n",
            "img_129.jpg\t1966AB08 \n",
            "img_130.jpg\t2534AB06 \n",
            "img_133.jpg\t3569AA04 \n",
            "img_137.jpg\t3079AM00 \n",
            "img_139.jpg\t1700AA05 \n",
            "img_140.jpg\t1417AX00 \n",
            "img_142.jpg\t5183AA12 \n",
            "img_144.jpg\t1709AA05 \n",
            "img_146.jpg\t9039AB08 \n",
            "img_147.jpg\t0153AB08 \n",
            "img_148.jpg\t1874AP00 \n",
            "img_149.jpg\t1428AL00 \n",
            "img_150.jpg\t9227AA08 \n",
            "img_151.jpg\t1579AA12 \n",
            "img_156.jpg\t6831AB08 \n",
            "img_157.jpg\t8594AA07 \n",
            "img_158.jpg\t0715AB07 \n",
            "img_159.jpg\t1973AA01 \n",
            "img_16.jpg\t1806AY00 \n",
            "img_162.jpg\t5220AA03 \n",
            "img_163.jpg\t7819AB08 \n",
            "img_164.jpg\t8146AB08 \n",
            "img_165.jpg\t6827AX00 \n",
            "img_166.jpg\t0368AS00 \n",
            "img_17.jpg\t3726AN00 \n",
            "img_172.jpg\t7526AB06 \n",
            "img_175.jpg\t1833AA12 \n",
            "img_179.jpg\t5778AR00 \n",
            "img_18.jpg\t0505BA00 \n",
            "img_181.jpg\t5003AA03 \n",
            "img_183.jpg\t2320AY00 \n",
            "img_185.jpg\t3699AR06 \n",
            "img_189.jpg\t0732AA05 \n",
            "img_190.jpg\t2119AA09 \n",
            "img_191.jpg\t5215AA12 \n",
            "img_193.jpg\t8839AY00 \n",
            "img_196.jpg\t7363AX00 \n",
            "img_197.jpg\t2928BA00 \n",
            "img_20.jpg\t6606AB07 \n",
            "img_200.jpg\t9174AZ00 \n",
            "img_202.jpg\t5620AB06 \n",
            "img_203.jpg\t0427AY00 \n",
            "img_204.jpg\t1206AB06 \n",
            "img_207.jpg\t6934AA02 \n",
            "img_208.jpg\t4934AA12 \n",
            "img_209.jpg\t2539AA12 \n",
            "img_210.jpg\t7923AN00 \n",
            "img_211.jpg\t2336AA05 \n",
            "img_212.jpg\t0561AC08 \n",
            "img_213.jpg\t5521AA05 \n",
            "img_215.jpg\t4975AB06 \n",
            "img_216.jpg\t4139AB08 \n",
            "img_221.jpg\t9530AA07 \n",
            "img_227.jpg\t9688AR10 \n",
            "img_228.jpg\t2116AX80 \n",
            "img_229.jpg\t3850AA08 \n",
            "img_233.jpg\t7870AA06 \n",
            "img_235.jpg\t2352AA09 \n",
            "img_236.jpg\t6699AG00 \n",
            "img_24.jpg\t3775AU00 \n",
            "img_240.jpg\t5438AA06 \n",
            "img_243.jpg\t5957AM03 \n",
            "img_244.jpg\t1347AB06 \n",
            "img_246.jpg\t7360AR00 \n",
            "img_248.jpg\t6602AB07 \n",
            "img_249.jpg\t5780AB08 \n",
            "img_25.jpg\t4914AB06 \n",
            "img_250.jpg\t3256AY00 \n",
            "img_252.jpg\t3614AR00 \n",
            "img_253.jpg\t9476AR00 \n",
            "img_259.jpg\t1517AA09 \n",
            "img_26.jpg\t5949AB08 \n",
            "img_264.jpg\t8784AA08 \n",
            "img_265.jpg\t0914BA00 \n",
            "img_266.jpg\t6831AB08 \n",
            "img_267.jpg\t9617AG00 \n",
            "img_269.jpg\t7110AM00 \n",
            "img_27.jpg\t5366AB08 \n",
            "img_270.jpg\t6878AU00 \n",
            "img_272.jpg\t8904AA07 \n",
            "img_274.jpg\t0099AC08 \n",
            "img_277.jpg\t0624AB06 \n",
            "img_278.jpg\t1693AB06 \n",
            "img_279.jpg\t0141AC06 \n",
            "img_283.jpg\t1957BA00 \n",
            "img_284.jpg\t5649AA08 \n",
            "img_285.jpg\t1508AA09 \n",
            "img_286.jpg\t0848AV00 \n",
            "img_29.jpg\t8485AB06 \n",
            "img_291.jpg\t4296AG00 \n",
            "img_292.jpg\t7160AS00 \n",
            "img_293.jpg\t9879AP00 \n",
            "img_294.jpg\t2950AB08 \n",
            "img_295.jpg\t1365AA09 \n",
            "img_297.jpg\t6025AA12 \n",
            "img_298.jpg\t6454AB07 \n",
            "img_299.jpg\t5555AA07 \n",
            "img_302.jpg\t7145AA12 \n",
            "img_304.jpg\t0225AC06 \n",
            "img_306.jpg\t3189AB07 \n",
            "img_308.jpg\t4602AA12 \n",
            "img_309.jpg\t8798AP00 \n",
            "img_31.jpg\t8550AS00 \n",
            "img_312.jpg\t6281AB07 \n",
            "img_313.jpg\t3673AB06 \n",
            "img_314.jpg\t9025AX00 \n",
            "img_316.jpg\t0765AB06 \n",
            "img_318.jpg\t0024AU00 \n",
            "img_319.jpg\t2429AA03 \n",
            "img_32.jpg\t4111AA01 \n",
            "img_320.jpg\t7309AG00 \n",
            "img_322.jpg\t4118AU00 \n",
            "img_325.jpg\t8591AA06 \n",
            "img_33.jpg\t1406BA00 \n",
            "img_34.jpg\t5700AB08 \n",
            "img_35.jpg\t0908AU00 \n",
            "img_37.jpg\t6206AA07 \n",
            "img_4.jpg\t4710AA03 \n",
            "img_40.jpg\t3337AV00 \n",
            "img_41.jpg\t5391AC06 \n",
            "img_43.jpg\t4766AA06 \n",
            "img_48.jpg\t9890AX00 \n",
            "img_49.jpg\t9150AB06 \n",
            "img_51.jpg\t9322AA07 \n",
            "img_53.jpg\t2988AX00 \n",
            "img_55.jpg\t3540AB08 \n",
            "img_56.jpg\t1901AA05 \n",
            "img_58.jpg\t2389AB07 \n",
            "img_60.jpg\t9189AA03 \n",
            "img_61.jpg\t3221AM00 \n",
            "img_63.jpg\t7180AG00 \n",
            "img_65.jpg\t8348AA02 \n",
            "img_67.jpg\t1949AA01 \n",
            "img_68.jpg\t8520AN00 \n",
            "img_69.jpg\t5588AY00 \n",
            "img_7.jpg\t5601AA08 \n",
            "img_70.jpg\t1693AB06 \n",
            "img_73.jpg\t5600AX00 \n",
            "img_74.jpg\t4882AA12 \n",
            "img_76.jpg\t2659AA09 \n",
            "img_8.jpg\t5688AM00 \n",
            "img_84.jpg\t3998AA06 \n",
            "img_85.jpg\t6095AV00 \n",
            "img_86.jpg\t1261AA10 \n",
            "img_87.jpg\t8271AB06 \n",
            "img_88.jpg\t9824AA07 \n",
            "img_89.jpg\t4250AA06 \n",
            "img_9.jpg\t7438AB08 \n",
            "img_91.jpg\t0714AZ08 \n",
            "img_94.jpg\t0540AX00 \n",
            "img_96.jpg\t1964AB08 \n",
            "img_97.jpg\t2734AB07 \n",
            "img_98.jpg\t9910AA08 \n",
            "img_99.jpg\t1480AV00 \n",
            "img_328.jpg\t5917AA09 \n",
            "img_330.jpg\t5022AA03 \n",
            "img_331.jpg\t1814AY00 \n",
            "img_333.jpg\t1833AA03 \n",
            "img_334.jpg\t3673AB06 \n",
            "img_335.jpg\t0483AB08 \n",
            "img_336.jpg\t3463AA02 \n",
            "img_337.jpg\t6584AB07 \n",
            "img_339.jpg\t8457AK00 \n",
            "img_340.jpg\t7664AA08 \n",
            "img_341.jpg\t3292AU00 \n",
            "img_342.jpg\t0078AZ00 \n",
            "img_343.jpg\t3578AB06 \n",
            "img_344.jpg\t7126AL00 \n",
            "img_345.jpg\t6930AE00 \n",
            "img_346.jpg\t4052AA05 \n",
            "img_347.jpg\t9770AA06 \n",
            "img_348.jpg\t8862AB06 \n",
            "img_349.jpg\t6074AA02 \n",
            "img_350.jpg\t5678AA07 \n",
            "img_351.jpg\t2228AA12 \n",
            "img_352.jpg\t9639AS00 \n",
            "img_353.jpg\t8814AV00 \n",
            "img_354.jpg\t2900AA12 \n",
            "img_355.jpg\t0940AC08 \n",
            "img_357.jpg\t1575AS00 \n",
            "img_358.jpg\t4363AB07 \n",
            "img_363.jpg\t8143AV00 \n",
            "img_364.jpg\t7783AB08 \n",
            "img_366.jpg\t0904AB06 \n",
            "img_368.jpg\t0004AY00 \n",
            "img_370.jpg\t1762AB08 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd deep-text-recognition-benchmark # moving to the deep text recognition benchmark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBT6H2AJLG9H",
        "outputId": "d9f37e0f-6286-49c8-999c-a06dafabb7d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deep-text-recognition-benchmark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#getting the pretrained model from google drive (i got this pretrained model from he freecodecamp articule )\n",
        "import gdown\n",
        "file_id = \"1b59rXuGGmKne1AuHnkgDzoYgKeETNMv9\"\n",
        "\n",
        "# Construct the download URL\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "# Download the file\n",
        "output = \"/content/TPS-ResNet-BiLSTM-Attn.pth\"  # Change this to your desired filename and extension\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "o2EoM6b4H-4n",
        "outputId": "770b4fe7-d1e7-4770-b77b-b73a5c7d0e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1b59rXuGGmKne1AuHnkgDzoYgKeETNMv9\n",
            "From (redirected): https://drive.google.com/uc?id=1b59rXuGGmKne1AuHnkgDzoYgKeETNMv9&confirm=t&uuid=fd80254a-a268-417f-bbe5-3c068613f3ff\n",
            "To: /content/TPS-ResNet-BiLSTM-Attn.pth\n",
            "100%|██████████| 199M/199M [00:13<00:00, 15.0MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/TPS-ResNet-BiLSTM-Attn.pth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the lmdb module\n",
        "import lmdb"
      ],
      "metadata": {
        "id": "n8Yhn5hLbeN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transofming the dataset to lmdb format\n",
        "!python create_lmdb_dataset.py \\data /content/drive/MyDrive/rim_ai_competition/deep-text-recognition-benchmark/data/labels.txt \\lmbd_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLPprLCDtMwd",
        "outputId": "acbdf8cf-8936-48f0-913b-e1cff3ded8e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "204\n",
            "Created dataset with 204 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**running the train.py file to train the model:**\n",
        "\n",
        "\n",
        "*   train_data and val_data : path to the lmdb_output folder containing the cropped train images using yolo\n",
        "*   saved_model : path to the pretrained easyocr model we downloaded form google drive\n",
        "*   num_iterations : we trained the model for 300 iterations then for another 100 iterations so 400 iterations might be just enough\n"
      ],
      "metadata": {
        "id": "uS3ejAo1zOWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --train_data /content/drive/MyDrive/rim_ai_competition/deep-text-recognition-benchmark/lmbd_output --valid_data /content/drive/MyDrive/rim_ai_competition/deep-text-recognition-benchmark/lmbd_output  --select_data \"/\" --batch_ratio 1.0 --Transformation TPS --FeatureExtraction ResNet --SequenceModeling BiLSTM --Prediction Attn --batch_size 16 --data_filtering_off --workers 0 --batch_max_length 1000 --num_iter 400 --valInterval 5 --data_filtering_off --saved_model /content/TPS-ResNet-BiLSTM-Attn.pth"
      ],
      "metadata": {
        "id": "vUGca8sAPyPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**running inference on the test data :**\n",
        "\n",
        "\n",
        "\n",
        "*   saved_model:the path to the fine_tuned easy ocr model\n",
        "*   image_folder : path to the folder containing the cropped test images using yolo\n",
        "\n",
        "\n",
        "**note :** after running inference a log_demo_result.txt file will be created containing the predictions it should be in the deeptext recognition folder\n",
        "\n"
      ],
      "metadata": {
        "id": "4_erLTXKBYh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python demo.py --Transformation TPS --FeatureExtraction ResNet --SequenceModeling BiLSTM --Prediction Attn --image_folder /content/drive/MyDrive/rim_ai_competition/croppet_test_number --saved_model /content/drive/MyDrive/rim_ai_competition/deep-text-recognition-benchmark/saved_models/best_model/best_norm_ED.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l72h6C1Pm7h1",
        "outputId": "90f11b47-7d85-4239-c93c-bf6b567de169",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model input parameters 32 100 20 1 512 256 38 25 TPS ResNet BiLSTM Attn\n",
            "loading pretrained model from /content/drive/MyDrive/rim_ai_competition/deep-text-recognition-benchmark/saved_models/best_model/best_norm_ED.pth\n",
            "/content/drive/MyDrive/rim_ai_competition/deep-text-recognition-benchmark/demo.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(opt.saved_model, map_location=device))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "--------------------------------------------------------------------------------\n",
            "image                    \tpredicted_labels         \tconfidence score\n",
            "--------------------------------------------------------------------------------\n",
            "img_2.jpg                \t5896AA08                 \t0.9679\n",
            "img_3.jpg                \t3395AB07                 \t0.9874\n",
            "img_5.jpg                \t7145AL00                 \t0.9338\n",
            "img_6.jpg                \t0722AB03                 \t0.9921\n",
            "img_13.jpg               \t1112AB06                 \t0.9926\n",
            "img_19.jpg               \t0195A006                 \t0.0774\n",
            "img_22.jpg               \t0714AZ08                 \t0.3164\n",
            "img_28.jpg               \t3791AB06                 \t0.9672\n",
            "img_30.jpg               \t7133AB07                 \t0.9881\n",
            "img_38.jpg               \t7080AY00                 \t0.9952\n",
            "img_45.jpg               \t6361AV00                 \t0.9219\n",
            "img_46.jpg               \t0081AZ00                 \t0.9508\n",
            "img_47.jpg               \t5853AR08                 \t0.5618\n",
            "img_50.jpg               \t1418AY00                 \t0.9956\n",
            "img_52.jpg               \t4336AX00                 \t0.9968\n",
            "img_57.jpg               \t7734AB06                 \t0.9932\n",
            "img_59.jpg               \t0962AB03                 \t0.9892\n",
            "img_66.jpg               \t5546AB06                 \t0.9524\n",
            "img_72.jpg               \t0141AC06                 \t0.9858\n",
            "img_75.jpg               \t6084AY00                 \t0.9930\n",
            "img_77.jpg               \t4153AB07                 \t0.7336\n",
            "img_78.jpg               \t8856AA02                 \t0.9612\n",
            "img_80.jpg               \t1114AA01                 \t0.9774\n",
            "img_81.jpg               \t7365AA02                 \t0.9256\n",
            "img_82.jpg               \t8594AA00                 \t0.2245\n",
            "img_83.jpg               \t6533AA07                 \t0.9895\n",
            "img_90.jpg               \t7121AA12                 \t0.9889\n",
            "img_92.jpg               \t2339AA09                 \t0.9906\n",
            "img_101.jpg              \t8944AA06                 \t0.9674\n",
            "img_104.jpg              \t5051AY00                 \t0.9963\n",
            "img_113.jpg              \t9838AA08                 \t0.5816\n",
            "img_114.jpg              \t8183AA07                 \t0.9906\n",
            "img_116.jpg              \t2450AX00                 \t0.9945\n",
            "img_117.jpg              \t4818AX00                 \t0.9946\n",
            "img_121.jpg              \t9687AB08                 \t0.9933\n",
            "img_122.jpg              \t6757AU00                 \t0.9968\n",
            "img_124.jpg              \t5215AA12                 \t0.9934\n",
            "img_125.jpg              \t4618AY00                 \t0.8359\n",
            "img_131.jpg              \t4922AB07                 \t0.9703\n",
            "img_134.jpg              \t7427AA06                 \t0.5757\n",
            "img_136.jpg              \t3673AB06                 \t0.9963\n",
            "img_141.jpg              \t5313AR00                 \t0.9744\n",
            "img_143.jpg              \t8896AA07                 \t0.9787\n",
            "img_145.jpg              \t5189AB07                 \t0.9858\n",
            "img_152.jpg              \t7639AG00                 \t0.9964\n",
            "img_153.jpg              \t7471AR00                 \t0.9506\n",
            "img_154.jpg              \t3959AA05                 \t0.9366\n",
            "img_168.jpg              \t2650AZ00                 \t0.9730\n",
            "img_169.jpg              \t8810AB08                 \t0.9894\n",
            "img_174.jpg              \t3807AA08                 \t0.9719\n",
            "img_176.jpg              \t5178AA07                 \t0.9896\n",
            "img_180.jpg              \t8303AA03                 \t0.9937\n",
            "img_182.jpg              \t0476AX00                 \t0.9933\n",
            "img_184.jpg              \t0478AB08                 \t0.9962\n",
            "img_186.jpg              \t2025AB06                 \t0.9260\n",
            "img_188.jpg              \t6956AU00                 \t0.9880\n",
            "img_194.jpg              \t0294AB07                 \t0.9665\n",
            "img_195.jpg              \t7037AA07                 \t0.9887\n",
            "img_198.jpg              \t1604AB06                 \t0.9960\n",
            "img_199.jpg              \t1987AA09                 \t0.9940\n",
            "img_201.jpg              \t5654AA08                 \t0.9155\n",
            "img_206.jpg              \t6353AA08                 \t0.9818\n",
            "img_214.jpg              \t6425AA06                 \t0.9914\n",
            "img_217.jpg              \t4890AY00                 \t0.9969\n",
            "img_226.jpg              \t2336AA05                 \t0.9942\n",
            "img_232.jpg              \t8978AY00                 \t0.9927\n",
            "img_234.jpg              \t4875AB06                 \t0.9860\n",
            "img_237.jpg              \t2619AA00                 \t0.9959\n",
            "img_238.jpg              \t7139AA07                 \t0.9802\n",
            "img_245.jpg              \t1901AA05                 \t0.7715\n",
            "img_256.jpg              \t2077AA05                 \t0.9252\n",
            "img_257.jpg              \t1085AC08                 \t0.6394\n",
            "img_258.jpg              \t3089AX00                 \t0.1901\n",
            "img_260.jpg              \t7338AA07                 \t0.9800\n",
            "img_261.jpg              \t2828AU00                 \t0.4933\n",
            "img_262.jpg              \t8411AA08                 \t0.9643\n",
            "img_263.jpg              \t1478AU00                 \t0.9979\n",
            "img_271.jpg              \t9984AA06                 \t0.5671\n",
            "img_273.jpg              \t7098AL00                 \t0.9798\n",
            "img_280.jpg              \t0614AB08                 \t0.9944\n",
            "img_281.jpg              \t5373AV00                 \t0.9880\n",
            "img_288.jpg              \t3244AX00                 \t0.9808\n",
            "img_289.jpg              \t1776AX00                 \t0.9940\n",
            "img_290.jpg              \t3292AC06                 \t0.8341\n",
            "img_296.jpg              \t9942AL00                 \t0.9935\n",
            "img_317.jpg              \t0701AP00                 \t0.5915\n",
            "img_321.jpg              \t0445AS00                 \t0.6248\n",
            "img_324.jpg              \t5370AK00                 \t0.9882\n",
            "img_327.jpg              \t2760AB06                 \t0.9870\n",
            "img_329.jpg              \t7982AA06                 \t0.9937\n",
            "img_332.jpg              \t5263AA03                 \t0.9920\n",
            "img_338.jpg              \t1975AA09                 \t0.9902\n",
            "img_356.jpg              \t2316AZ00                 \t0.9899\n",
            "img_359.jpg              \t1347AB06                 \t0.9936\n",
            "img_360.jpg              \t3383AB08                 \t0.9953\n",
            "img_361.jpg              \t4740AA02                 \t0.9712\n",
            "img_362.jpg              \t5586AA12                 \t0.9912\n",
            "img_365.jpg              \t3074AA00                 \t0.1425\n",
            "img_367.jpg              \t3792AA03                 \t0.9885\n",
            "img_369.jpg              \t8383AV00                 \t0.5291\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
